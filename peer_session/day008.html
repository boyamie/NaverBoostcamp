<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mentoring 240814</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1 {
            color: #333;
        }
        h2 {
            color: #555;
        }
        ul {
            list-style-type: disc;
            margin-left: 20px;
        }
    </style>
</head>
<body>
    <h1>Mentoring 240814</h1>

    <h2>1. Deep Learning Process Recap</h2>
    <p>
        Deep learning process에서 input과 output 구조를 보면, MNIST 데이터셋의 28x28 이미지를 벡터화시키면 784가 input으로 들어가고, output으로는 10차원이 나옵니다. 데이터를 통해 자연스럽게 학습을 진행하며, 이는 0부터 9까지의 숫자를 분류하는 것일 수도 있지만, 이미지를 통해 사람을 제거하는 등의 작업에서도 모델의 구성에 대한 힌트를 얻을 수 있습니다.
    </p>

    <h2>2. 강의 내용 Review</h2>
    <ul>
        <li><strong>PyTorch 사용:</strong> Custom Model - 기존에 정의되어 있는 모델을 이용하거나 조금 변형합니다.</li>
        <li><strong>Custom Dataset/Dataloader:</strong> 논문을 보고 이를 구현하는 것이 엔지니어의 역량을 나타냅니다. 대부분의 회사는 자체 데이터셋을 이용합니다.</li>
        <li><strong>Autograd 및 Optimizer:</strong> 모델 학습 및 최적화에 사용됩니다.</li>
        <li><strong>Hyperparameter Tuning:</strong> 모델의 성능을 최적화하기 위해 중요한 작업입니다.</li>
        <li><strong>다중 GPU 활용:</strong> 앞으로 찾아보면 좋은 키워드로는 PyTorch-Lightning, DeepSpeed 등이 있습니다. 이들은 Multi-GPU를 어떻게 효율적으로 학습시킬지를 다룹니다.</li>
    </ul>
    <p>
        회사에서는 보통 GPU가 4개 정도 주어지며, 업무 기간은 1주일입니다. A라는 사람이 1개의 GPU를 이용해서 10의 결과를 낸다면, B라는 사람은 4개의 GPU를 이용해서 100의 결과를 낼 수 있을 것입니다. 따라서 이론뿐만 아니라 주어진 자원을 얼마나 잘 활용할 수 있는지가 중요하며, GPU가 많고 실험을 많이 돌릴 수 있는 환경이 주어지면 더욱 좋습니다.
    </p>
</body>
</html>
</head>
<body>
    <h1>Multi-GPU 활용 및 PyTorch Lightning</h1>

    <h2>1. 다중 GPU 활용의 필요성</h2>
    <p>
        딥러닝 모델이 점점 더 복잡해지고 대규모 데이터셋을 처리해야 할 때, 단일 GPU로는 처리 속도와 메모리 측면에서 한계가 발생할 수 있습니다. 이때 다중 GPU를 활용하면 다음과 같은 이점이 있습니다:
    </p>
    <ul>
        <li><strong>병렬 처리:</strong> 여러 GPU가 병렬로 연산을 수행함으로써 학습 시간을 단축할 수 있습니다.</li>
        <li><strong>메모리 확장:</strong> 각 GPU에 데이터를 분산하여 더 큰 모델을 처리할 수 있습니다.</li>
        <li><strong>성능 향상:</strong> GPU를 추가로 활용하여 더 큰 배치 크기(batch size)로 학습을 진행할 수 있습니다.</li>
    </ul>

    <h2>2. PyTorch에서 Multi-GPU 활용</h2>

    <h3>2.1 DataParallel</h3>
    <p>
        <code>DataParallel</code>은 가장 기본적인 Multi-GPU 사용 방법입니다. 모델을 여러 GPU에 분산시키고, 각 GPU에서 병렬로 학습을 진행한 뒤, 결과를 집계하여 업데이트합니다.
    </p>
    <pre><code>import torch
import torch.nn as nn

# 모델 정의
model = MyModel()

# 모델을 여러 GPU에 분산
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)

# 모델을 GPU로 이동
model = model.to('cuda')

# 이후 학습 루프에서 그대로 사용 가능</code></pre>
    <p><strong>장점:</strong> 사용이 간편하며, 코드 수정이 거의 필요 없습니다.</p>
    <p><strong>단점:</strong> GPU 간의 통신 오버헤드가 존재하며, 성능 최적화 측면에서 제한이 있을 수 있습니다.</p>

    <h3>2.2 DistributedDataParallel</h3>
    <p>
        <code>DistributedDataParallel</code>(DDP)은 <code>DataParallel</code>보다 더 성능이 좋은 방법입니다. 각 GPU가 독립적으로 모델을 학습하고, 그 결과를 동기화하여 업데이트합니다.
    </p>
    <pre><code>import torch
import torch.nn as nn
import torch.distributed as dist

# DDP 설정
dist.init_process_group(backend='nccl')

# 모델 정의
model = MyModel().to('cuda')

# 모델을 DDP로 감싸기
model = nn.parallel.DistributedDataParallel(model)

# 이후 학습 루프에서 그대로 사용 가능</code></pre>
    <p><strong>장점:</strong> <code>DataParallel</code>에 비해 성능이 우수하며, GPU 간의 통신 오버헤드가 적습니다. 대규모 분산 학습에 적합합니다.</p>
    <p><strong>단점:</strong> 설정이 조금 더 복잡하며, DDP에 맞춘 코드를 작성해야 할 수 있습니다.</p>

    <h2>3. PyTorch Lightning</h2>
    <p>
        PyTorch Lightning은 PyTorch 기반의 고수준 라이브러리로, 코드의 복잡성을 줄이고, Multi-GPU 학습을 포함한 여러 가지 학습 전략을 쉽게 구현할 수 있도록 도와줍니다.
    </p>

    <h3>3.1 PyTorch Lightning을 이용한 Multi-GPU 학습</h3>
    <p>
        PyTorch Lightning에서 Multi-GPU 학습을 위해 필요한 코드는 매우 간단합니다. 모델을 <code>pl.LightningModule</code>로 정의하고, 학습 시에 <code>Trainer</code>에 <code>gpus</code> 인자를 설정하면 됩니다.
    </p>
    <pre><code>import pytorch_lightning as pl

class MyLightningModel(pl.LightningModule):
    def __init__(self):
        super(MyLightningModel, self).__init__()
        self.model = MyModel()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        loss = nn.functional.cross_entropy(y_hat, y)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

# 모델 정의
model = MyLightningModel()

# Trainer 정의 및 Multi-GPU 학습
trainer = pl.Trainer(gpus=4, accelerator='ddp')
trainer.fit(model, train_dataloader)</code></pre>
    <p><strong>장점:</strong> 코드의 가독성과 유지보수성을 높여주며, 복잡한 학습 루프를 간결하게 작성할 수 있습니다.</p>
    <p><strong>단점:</strong> 기존 PyTorch 코드와 다소 다르기 때문에 처음에 익숙해지는 데 시간이 걸릴 수 있습니다.</p>

    <h2>4. Multi-GPU 학습 효율화</h2>
    <p>
        다중 GPU 학습의 효율성을 극대화하기 위해 고려해야 할 몇 가지 사항이 있습니다:
    </p>
    <h3>4.1 배치 크기 최적화</h3>
    <p>
        배치 크기는 각 GPU가 효율적으로 계산을 수행할 수 있도록 조정해야 합니다. 일반적으로 GPU의 메모리 용량에 따라 적절한 배치 크기를 설정해야 하며, 배치 크기를 늘리면 학습 속도가 빨라질 수 있습니다.
    </p>
    <h3>4.2 통신 오버헤드 최소화</h3>
    <p>
        GPU 간의 통신 오버헤드를 줄이기 위해 <code>DistributedDataParallel</code>을 사용하는 것이 좋습니다. 이 방법은 GPU 간의 동기화를 효과적으로 처리하며, 학습 속도를 높이는 데 유리합니다.
    </p>
    <h3>4.3 GPU 활용도 모니터링</h3>
    <p>
        학습 중에는 GPU의 사용률과 메모리 사용량을 모니터링하여 각 GPU가 최대한 효율적으로 사용되고 있는지 확인해야 합니다. 이 정보는 <code>nvidia-smi</code> 명령어를 사용하여 실시간으로 확인할 수 있습니다.
    </p>
    <h3>4.4 최적화 및 튜닝</h3>
    <p>
        PyTorch Lightning에서는 다양한 최적화 기법(예: 혼합 정밀도 학습, Gradient Clipping 등)을 쉽게 적용할 수 있습니다. 이를 통해 학습 속도를 더욱 향상시킬 수 있습니다.
    </p>
</body>
</html>
</head>
<body>
    <h1>Machine Learning Study Guide</h1>

    <h2>1. 부캠에서 다루지 않는 Task들</h2>
    <p>부캠에서는 다루지 않는 몇 가지 중요한 주제들이 있습니다. 이 주제들은 별도로 공부해야 할 필요가 있습니다.</p>
    <ul>
        <li><strong>NeRF (Neural Radiance Fields):</strong> Neural View Synthesis의 일종으로, 3D 장면에서 새로운 관점의 이미지를 합성하는 기법입니다.</li>
        <li><strong>Domain Adaptation:</strong> 모델이 학습한 데이터셋과 실제 사용하는 데이터셋 간의 차이를 줄이기 위한 기법입니다.</li>
        <li><strong>생성 모델 (Generative Models):</strong> 데이터의 분포를 학습하고 새로운 데이터를 생성할 수 있는 모델입니다. 예: GAN, VAE.</li>
        <li><strong>따로 공부하는 과정이 필요 (논문):</strong> 이 주제들은 깊이가 있으므로, 논문을 통해 학습하는 것이 중요합니다.</li>
    </ul>

    <h2>2. 새로운 분야에 접근하는 방법</h2>
    <p>새로운 연구 분야에 접근할 때는 Survey 논문을 읽는 것이 좋습니다.</p>
    <ul>
        <li><strong>Survey 논문:</strong> 특정 분야에 대한 전반적인 개요를 제공하며, 최신 연구 동향을 이해하는 데 도움을 줍니다.</li>
        <li><strong>최신 논문 및 인용수 높은 논문:</strong> 최신 연구의 흐름을 빠르게 파악할 수 있습니다.</li>
    </ul>

    <h2>3. 최신 논문 읽기</h2>
    <p>분야의 기본적인 이해가 잡힌 후에는, 검색을 통해 최신 논문을 읽는 것이 중요합니다.</p>
    <ul>
        <li><strong>탑티어 학회:</strong> CVPR, ICCV, ECCV, AAAI, NeurIPS, ICLR, ICML 등의 학회 논문을 우선적으로 살펴보는 것이 좋습니다.</li>
        <li><strong>논문 검색:</strong> <a href="https://aideadlin.es/?sub=CV" target="_blank">aideadlin.es</a>와 같은 사이트를 통해 컴퓨터 비전 관련 최신 논문을 검색하고 읽을 수 있습니다.</li>
    </ul>
</body>
</html>
</head>
<body>
    <h1>논문 Review & Study 방법</h1>

    <h2>컴퓨터 비전 분야 스터디</h2>
    <p>-> 일단은 기초를 다져야 할 시기! (주 1회 기준)</p>

    <p>컴퓨터 비전 전반적인 모델에 대한 학습을 목표로 함. -> CNN 계열, RNN 계열, Transformer 모델</p>
    <ul>
        <li>AlexNet</li>
        <li>VggNet</li>
        <li>ResNet</li>
        <li>RNN (Vanilla RNN)</li>
        <li>LSTM</li>
        <li>Transformer</li>
    </ul>
    <p>모델을 구현하고, 특정 데이터셋에 대해 성능을 낼 수 있도록 스터디 진행.</p>
    <p>
        - <a href="https://kmhana.tistory.com/3" target="_blank">https://kmhana.tistory.com/3</a><br>
        (조금 옛날 자료여서 논문들에 대한 중요도는 지금과는 많이 다름. 그냥 흐름에 대해서만 확인!)
    </p>

    <h2>논문의 구조</h2>
    <ul>
        <li><strong>Abstract:</strong> 논문이 어떤 문제를 풀고, 어떤 기여를 했는지 설명.</li>
        <li><strong>Introduction:</strong> 조금 더 길게 논문이 어떤 문제를 풀고 어떤 기여를 했는지 설명.</li>
        <li><strong>Related Works:</strong> 작성한 논문과 관련있는 선행 연구들을 설명.</li>
        <li><strong>Method:</strong> 본인의 방법론에 대해 자세하게 설명.</li>
        <li><strong>Experiments:</strong> 본인의 방법론이 옳은지 실험을 통해 정당화하는 부분(주로 타 모델, 연구와의 비교).</li>
        <li><strong>Ablation:</strong> 실험에서 추가적인 요소를 제거하거나 추가하여 그 영향을 분석.</li>
        <li><strong>Conclusion:</strong> 결론적으로 우리는 잘했다는 점을 강조하며 마무리.</li>
    </ul>

    <p>따라서 논문을 읽으면, 다음 항목들에 대해서는 안보고도 말할 수 있어야 함:</p>
    <ol>
        <li>이 논문이 어떠한 문제를 인식했고 해결하려 했는지? (Motivation)</li>
        <li>인식한 문제를 어떤 방식으로 해결 했는지? (Contribution) + 참신한지? (Novelty)</li>
    </ol>
</body>
</html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>최신 트렌드를 팔로우 하는 방법</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1 {
            color: #333;
            font-size: 2em;
            margin-bottom: 20px;
        }
        ul {
            list-style-type: disc;
            margin-left: 20px;
            margin-bottom: 10px;
        }
        a {
            color: #007bff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>최신 트렌드를 팔로우 하는 방법</h1>

    <ul>
        <li>유튜브, 트위터, 페이스북, <a href="https://paperswithcode.com/" target="_blank">Paperswithcode</a> 등등</li>
        <li>트위터: <a href="https://twitter.com/_akhaliq?lang=en" target="_blank">https://twitter.com/_akhaliq?lang=en</a></li>
        <li>카카오톡 오픈챗 (인공지능 키워드)</li>
        <li>Reddit, Blog</li>
        <li>Paperswithcode: <a href="https://paperswithcode.com/" target="_blank">https://paperswithcode.com/</a></li>
        <li>Hugging Face: <a href="https://huggingface.co/papers" target="_blank">https://huggingface.co/papers</a></li>
        <li>Slack에 AI 트렌드 올리기? (인서님이 만들어주신다고 합니다)</li>
    </ul>

</body>
</html>ㅍ
