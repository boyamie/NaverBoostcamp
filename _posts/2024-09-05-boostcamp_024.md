---
title: "Boostcamp AI Tech (Day 024)"
date: 2024-09-05
layout: post
tags: [Naver Boostcamp, daily report]
---

## 1. 피어세션 시간 DDPM
![](https://velog.velcdn.com/images/boyamie_/post/548b19c8-7ef8-4c08-b4b2-477b942c8b1f/image.png)
DDPM(Denoising Diffusion Probabilistic Models)은 생성 모델의 한 종류로, 노이즈를 점진적으로 추가한 데이터에서 원래 데이터를 복원하는 방식으로 학습 및 샘플링을 진행합니다. 이 과정은 두 단계로 나눌 수 있습니다: **Forward Process**와 **Reverse Process**. 아래에서는 수학적으로 각 과정을 설명하겠습니다.

### 1. **Forward Process (노이즈 추가 단계)**

Forward Process에서는 원래 데이터에 점진적으로 가우시안 노이즈를 추가합니다. 그림에서는 각 타임스텝 $$ T $$에서 데이터에 노이즈가 더해지며 점점 퍼지는 분포를 볼 수 있습니다.

- **수식**: 주어진 데이터 $$ x_0 $$에 대해, Forward Process는 각 시간 $$ t $$에서 데이터 $$ x_t $$를 아래와 같이 정의된 노이즈 추가 과정에 따라 생성합니다:
  
  $$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$$
  
  여기서 $$ \beta_t $$는 각 타임스텝에서의 노이즈의 크기를 나타냅니다. 이 노이즈는 가우시안 분포 $$ \mathcal{N} $$에 따라 추가되며, $$ x_{t-1} $$에 점점 더 많은 노이즈가 추가됨에 따라 $$ x_t $$는 원래 데이터 $$ x_0 $$와 멀어지게 됩니다.

#### **Forward Process에서 중요한 수식**
Forward Process는 일반적으로 데이터에 노이즈를 추가하는 과정입니다. $$ x_0 $$에 대해, $$ x_T $$는 순수한 노이즈로 변하게 됩니다. 이를 요약하면:
  
  \[
  q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)
  \]
  
  여기서, $$ \alpha_t = 1 - \beta_t $$, $$ \bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s $$입니다. 이 수식은 데이터가 시간이 지남에 따라 점진적으로 노이즈로 변하는 과정을 설명합니다.

### 2. **Reverse Process (노이즈 제거 단계)**

Reverse Process는 Forward Process에서 노이즈가 추가된 데이터를 다시 원래 데이터로 복원하는 과정입니다. 이 과정은 생성된 노이즈를 거꾸로 추정하여 원래 데이터를 복원하는 방식으로 동작합니다.

- **수식**: Reverse Process는 $$ p_\theta(x_{t-1} | x_t) $$로 정의되며, 이는 Forward Process의 반대 과정입니다. 이 때 각 스텝에서의 복원은 다음과 같은 가우시안 분포를 따릅니다:
  
  \[
  p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
  \]
  
  여기서 $$ \mu_\theta $$는 학습된 모델에 의해 예측된 평균이며, $$ \Sigma_\theta $$는 가우시안의 분산을 의미합니다. 학습 과정에서는 Forward Process에서의 노이즈를 기반으로, Reverse Process에서 어떻게 원래 데이터를 복원할지 학습합니다.

#### **Loss Function (손실 함수)**
Reverse Process에서 중요한 점은 모델이 각 스텝에서 예측하는 노이즈가 얼마나 정확한지입니다. 이때 손실 함수는 예측된 노이즈 $$ \epsilon_\theta(x_t, t) $$와 실제 노이즈 $$ \epsilon $$ 간의 차이를 최소화하는 방식으로 정의됩니다:

\[
L_t = \mathbb{E}_{x_0, \epsilon, t}\left[ \|\epsilon - \epsilon_\theta(x_t, t)\|^2 \right]
\]

이 손실 함수는 시간에 따른 노이즈 예측의 오차를 최소화하며, 모델이 점차적으로 정확하게 노이즈를 제거하는 방향으로 학습됩니다.

### 3. **이동 과정의 시각적 설명**
위 그림에서는 각 타임스텝 $$ T $$에서 데이터가 어떻게 변해가는지 보여줍니다:
- $$ T = 0 $$: 원래 데이터 분포.
- $$ T = 1 $$부터 $$ T = 8 $$까지: 시간이 지나면서 데이터에 노이즈가 더해지고, 데이터가 점점 퍼지며 가우시안 노이즈에 가까워집니다.
- $$ T $$가 증가할수록 데이터가 가우시안 분포를 따르며 노이즈가 많은 상태로 변합니다.

### 4. **모델의 역할**
DDPM의 핵심 목표는 Forward Process를 통해 점진적으로 노이즈가 추가된 데이터를 보고, Reverse Process에서 그 노이즈를 제거하여 원래 데이터를 복원하는 것입니다. 학습 과정에서 모델은 점진적으로 노이즈를 제거하는 방법을 학습하며, 손실 함수는 모델이 노이즈를 얼마나 정확히 제거했는지 평가합니다.

---

이 방식은 노이즈가 점진적으로 더해진 데이터를 학습하여 원래 데이터를 생성하는 데 매우 강력한 방법론입니다. DDPM은 특히 이미지를 생성하는 데 탁월한 성능을 발휘하며, 다양한 생성 모델에서 사용됩니다.DDPM(Denoising Diffusion Probabilistic Models)은 생성 모델의 한 종류로, 노이즈를 점진적으로 추가한 데이터에서 원래 데이터를 복원하는 방식으로 학습 및 샘플링을 진행합니다. 이 과정은 두 단계로 나눌 수 있습니다: **Forward Process**와 **Reverse Process**. 아래에서는 수학적으로 각 과정을 설명하겠습니다.

### 1. **Forward Process (노이즈 추가 단계)**

Forward Process에서는 원래 데이터에 점진적으로 가우시안 노이즈를 추가합니다. 그림에서는 각 타임스텝 $$ T $$에서 데이터에 노이즈가 더해지며 점점 퍼지는 분포를 볼 수 있습니다.

- **수식**: 주어진 데이터 $$ x_0 $$에 대해, Forward Process는 각 시간 $$ t $$에서 데이터 $$ x_t $$를 아래와 같이 정의된 노이즈 추가 과정에 따라 생성합니다:
  
  \[
  q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)
  \]
  
  여기서 $$ \beta_t $$는 각 타임스텝에서의 노이즈의 크기를 나타냅니다. 이 노이즈는 가우시안 분포 $$ \mathcal{N} $$에 따라 추가되며, $$ x_{t-1} $$에 점점 더 많은 노이즈가 추가됨에 따라 $$ x_t $$는 원래 데이터 $$ x_0 $$와 멀어지게 됩니다.

#### **Forward Process에서 중요한 수식**
Forward Process는 일반적으로 데이터에 노이즈를 추가하는 과정입니다. $$ x_0 $$에 대해, $$ x_T $$는 순수한 노이즈로 변하게 됩니다. 이를 요약하면:
  
  \[
  q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)
  \]
  
  여기서, $$ \alpha_t = 1 - \beta_t $$, $$ \bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s $$입니다. 이 수식은 데이터가 시간이 지남에 따라 점진적으로 노이즈로 변하는 과정을 설명합니다.

### 2. **Reverse Process (노이즈 제거 단계)**

Reverse Process는 Forward Process에서 노이즈가 추가된 데이터를 다시 원래 데이터로 복원하는 과정입니다. 이 과정은 생성된 노이즈를 거꾸로 추정하여 원래 데이터를 복원하는 방식으로 동작합니다.

- **수식**: Reverse Process는 $$ p_\theta(x_{t-1} | x_t) $$로 정의되며, 이는 Forward Process의 반대 과정입니다. 이 때 각 스텝에서의 복원은 다음과 같은 가우시안 분포를 따릅니다:
  
  \[
  p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
  \]
  
  여기서 $$ \mu_\theta $$는 학습된 모델에 의해 예측된 평균이며, $$ \Sigma_\theta $$는 가우시안의 분산을 의미합니다. 학습 과정에서는 Forward Process에서의 노이즈를 기반으로, Reverse Process에서 어떻게 원래 데이터를 복원할지 학습합니다.

#### **Loss Function (손실 함수)**
Reverse Process에서 중요한 점은 모델이 각 스텝에서 예측하는 노이즈가 얼마나 정확한지입니다. 이때 손실 함수는 예측된 노이즈 $$ \epsilon_\theta(x_t, t) $$와 실제 노이즈 $$ \epsilon $$ 간의 차이를 최소화하는 방식으로 정의됩니다:

\[
L_t = \mathbb{E}_{x_0, \epsilon, t}\left[ \|\epsilon - \epsilon_\theta(x_t, t)\|^2 \right]
\]

이 손실 함수는 시간에 따른 노이즈 예측의 오차를 최소화하며, 모델이 점차적으로 정확하게 노이즈를 제거하는 방향으로 학습됩니다.

### 3. **이동 과정의 시각적 설명**
위 그림에서는 각 타임스텝 $$ T $$에서 데이터가 어떻게 변해가는지 보여줍니다:
- $$ T = 0 $$: 원래 데이터 분포.
- $$ T = 1 $$부터 $$ T = 8 $$까지: 시간이 지나면서 데이터에 노이즈가 더해지고, 데이터가 점점 퍼지며 가우시안 노이즈에 가까워집니다.
- $$ T $$가 증가할수록 데이터가 가우시안 분포를 따르며 노이즈가 많은 상태로 변합니다.

### 4. **모델의 역할**
DDPM의 핵심 목표는 Forward Process를 통해 점진적으로 노이즈가 추가된 데이터를 보고, Reverse Process에서 그 노이즈를 제거하여 원래 데이터를 복원하는 것입니다. 학습 과정에서 모델은 점진적으로 노이즈를 제거하는 방법을 학습하며, 손실 함수는 모델이 노이즈를 얼마나 정확히 제거했는지 평가합니다.
