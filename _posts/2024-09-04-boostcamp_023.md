---
title: "Boostcamp AI Tech (Day 023)"
date: 2024-09-04
layout: post
tags: [Naver Boostcamp, daily report]
---

# 과제4 stable-dreamfusion
![image](https://github.com/user-attachments/assets/7630e0d3-0831-4a83-b877-7b454c80f347)

DreamFusion (ICLR 2023 Oral) Text가 입력으로 주어졌을 때 3D를 생성할 수 있는 방법론입니다! 3D represntation은 Instant-NGP 사용(다양한 모델 활용 가능)합니다. 저는 NeRF를 사용했습니다. Loss function은 Score Distillation (SDS) loss를 사용합니다.

SDS Loss는 무엇일까요 ?

기존 diffusion models은 수식 전개를 통해, noise prediction 만으로 diffusion model 업데이트가 가능했습니다! (e.g., DDPM).

![image](https://github.com/user-attachments/assets/e028ca94-4d0f-4c19-9a28-b9d0be3e87af)

condition y를 입력으로 받아, conditioning input을 cross-attention layers에 적용해 이미지 생성 시 condition을 반영합니다(e.g., Stable Diffusion).

![image](https://github.com/user-attachments/assets/5f102146-ad9c-4502-8148-68c0b95d815f)

SDS loss는 pre-trained Stable Diffusion(구현에서는 Stable Diffusion을 사용하지만 아래 figure에서는 Imagen 모델 사용)을 활용하여 text prompt 만으로 3D 생성이 가능하게 하는 방법론이랍니다.

![image](https://github.com/user-attachments/assets/8a2d4e40-88c2-46f3-9aa5-c26d54d7ecba)

먼저 필요한 package와 dependency들을 설치해줍니다.
DreamFusion의 오픈소스 버전인 Stable-Dreamfusion 코드를 설치해주고, diffusion을 활용하기 위한 추가적인 라이브러리(e.g., diffusers)도 설치해줍니다.

### 1. 클래스 초기화 (Initializing the Class with Super)
`StableDiffusion` 클래스의 `__init__` 메소드에서는 부모 클래스의 생성자를 호출해야 한다. 이렇게 하는 이유는 상속된 속성을 초기화하고, 클래스가 올바르게 설정되도록 하기 위해서다. 

```python
# 클래스 초기화 코드
super(StableDiffusion, self).__init__()
```

### 2. `min_step`과 `max_step` 정의 (Defining `min_step` and `max_step`)
`min_step`과 `max_step`는 `t_range`와 `self.num_train_timesteps`에서 파생된 정수 값이다. `t_range`의 값에 `self.num_train_timesteps`를 곱한 후 정수로 변환해야 한다.

```python
# min_step과 max_step 정의 코드
self.min_step = int(self.num_train_timesteps * t_range[0])
self.max_step = int(self.num_train_timesteps * t_range[1])
```

### 3. `pred_rgb`를 Latents로 보간하기 (Interpolating `pred_rgb` to Latents)
`as_latent`이 `True`일 때, `pred_rgb` 이미지를 `(64, 64)` 해상도로 리사이징하고, 값을 `[-1, 1]` 범위로 조정해서 바로 latent로 처리해야 한다.

```python
# as_latent이 True일 때 pred_rgb를 latents로 보간하기
latents = F.interpolate(pred_rgb, size=(64, 64), mode='bilinear', align_corners=False)
latents = 2 * latents - 1
```

### 4. `as_latent`이 `False`일 때 이미지 인코딩 (Encoding Images when `as_latent` is `False`)
`as_latent`이 `False`일 때는 `pred_rgb` 이미지를 `(512, 512)` 해상도로 리사이징한 후, `encode_imgs` 메소드를 사용해 latents로 인코딩해야 한다.

```python
# as_latent이 False일 때 pred_rgb 리사이징 및 인코딩
pred_rgb_512 = F.interpolate(pred_rgb, size=(512, 512), mode='bilinear', align_corners=False)
latents = self.encode_imgs(pred_rgb_512)
```

### 5. Timestep `t` 샘플링 (Sampling Timestep `t`)
`min_step`과 `max_step` 범위 내에서 무작위로 timestep `t`를 샘플링해야 한다. 이 작업은 `torch.randint`를 사용해 수행할 수 있다.

```python
# Timestep t 샘플링 코드
t = torch.randint(self.min_step, self.max_step + 1, (1,), device=self.device)
```

### 6. Gradient 계산하기 (Calculating Gradient)
Gradient 계산을 위해, 예측된 노이즈와 실제 노이즈의 차이를 계산하고, 여기에 `w` 값을 곱해 최종 gradient를 구해야 한다.

```python
# Gradient 계산 코드
grad = w * (noise_pred - noise)
```

### 7. SDS Loss를 위한 Targets 설정하기 (Setting Targets for SDS Loss)
`targets` 변수는 latents에서 예측된 노이즈를 뺀 값으로 설정해야 한다. 이 작업은 보통 noisy latents에서 노이즈 예측 값을 빼고, alpha 값으로 스케일링하여 수행된다.

```python
# SDS Loss를 위한 targets 정의
targets = latents - grad
```
