---
title: "Boostcamp AI Tech (Day 006)"
date: 2024-08-12
layout: post
tags: [Naver Boostcamp, daily report]
---

[![Peer Session Badge](https://img.shields.io/badge/Peer%20Session-CC527A?style=flat)](../peer_session/day006.html)

# 1.강의듣기
## 1.Linear Classifier와 Loss Function
>리니어 분류기
- 리니어 분류기는 입력 데이터를 레이블 점수에 매핑하는 함수 `f`를 사용합니다.
- 마진 기반 손실과 교차 엔트로피 같은 손실 함수는 학습 과정에서 모델의 성능을 정량화합니다.
- 빠른 테스트와 적은 메모리 사용은 리니어 분류기의 주요 이점입니다.


- 리니어 분류기는 입력 데이터를 레이블 점수에 매핑하는 함수 `f`를 활용합니다.
- 매개변수적 접근을 통해 각 픽셀의 가중치 합계를 사용하여 클래스별 점수를 결정하며, 적절한 가중치 `W` 값을 설정해야 합니다.
- 손실 함수는 학습 과정에서 사용되며, 마진 기반 손실 및 교차 엔트로피 등의 방법이 있습니다.
- 리니어 분류는 빠른 테스트와 적은 메모리 사용이 장점입니다.

## 2.Softmax Classifier 
>소프트맥스 분류기
- 소프트맥스 분류기는 점수를 확률로 변환하여 클래스 간의 구분을 명확히 합니다.
- 점수는 0과 1 사이의 경계로 해석되어 특정 클래스에 속할 확률을 결정합니다.
- 점수 차이가 클수록 특정 클래스에 속할 확률이 높아지는 특징이 있습니다.
- Sigmoid 함수를 통해 클래스의 경계 점수도 정의할 수 있습니다.

- 소프트맥스 분류기는 점수의 유용성과 사용 방법을 설명합니다.
- 점수는 해석이 어려워서 0과 1 사이의 경계 점수를 얻어 확률로 해석할 수 있어야 합니다.
- Sigmoid 함수를 사용하여 클래스의 경계 점수를 정의하고, 소프트맥스 분류기가 확률을 올바르게 산출하는지 확인해야 합니다.

## 3.Loss Function의 정의와 종류 소개
>손실 함수
- 손실 함수는 모델의 성능을 정량화하고 패널티를 부여합니다.
- 예측이 정확할수록 손실이 감소하며, 크로스 엔트로피 손실은 예측 확률에 비례합니다.
- Kullback-Leibler 발산은 두 확률 분포의 차이를 측정하는 데 사용됩니다.

- 손실 함수는 머신러닝 모델의 성능을 정량화하며, 추정값과 기준값의 차이에 따라 패널티를 부여합니다.
- 마진 기반 손실에서는 예측이 정확할수록 손실이 작아지며, 잘못된 분류일 경우 패널티가 커집니다.
- 크로스 엔트로피 손실 함수는 예측 확률이 1에 가까워질수록 손실이 감소합니다.
- Kullback-Leibler 발산은 두 확률 분포의 차이를 측정합니다.

## 4.최적화의 필요성과 방법에 대한 설명
>최적화
- 최적화는 매개변수 `W`의 값을 결정하는 과정으로, 반복적으로 업데이트됩니다.
- 그레이디언트 강하법을 통해 비용 함수의 기울기를 계산하고 이를 바탕으로 매개변수를 조정합니다.
- 최적화 과정은 예측값이 기준값에 근접할 때까지 반복됩니다.
- 기계 학습에서는 스케일 문제를 다루기 위한 다양한 최적화 방법이 필요합니다.

- 최적화는 매개변수 `W`의 값을 결정하는 중요한 문제이며, 머신러닝은 데이터 기반 접근 방식입니다.
- 그레이디언트 강하법을 사용하면 비용 함수 `J(Θ)`의 기울기를 계산하여 최적화할 수 있습니다.

## 5.ML과 최적화 과정
- Gradient Descent 최적화에서는 로컬 최적점이나 새들 포인트가 존재할 수 있으며, 대규모 데이터셋에서는 시간이 오래 걸릴 수 있습니다.
- Stochastic Gradient Descent 최적화는 훈련 예제의 하위집합에 대해 무작위로 샘플링하여 효율적으로 기울기를 계산합니다.

#2. 퀴즈 오답노트

### Softmax 함수와 수치적 불안정성

#### Softmax 함수 개요
Softmax 함수는 머신러닝, 특히 다중 클래스 분류 문제에서 널리 사용되는 활성화 함수입니다. 주어진 입력 값들의 집합에서 각 값의 지수(exp)를 계산하고, 이 지수들의 합으로 나눈 값을 출력으로 반환합니다. 이 과정은 입력 값을 확률 분포로 변환하는 역할을 합니다. 예를 들어, 여러 클래스 중 특정 클래스에 속할 확률을 계산할 때 사용됩니다.

Softmax 함수의 정의는 다음과 같습니다:

$${softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

여기서 $$z_i$$는 클래스 $$i$$에 대한 로짓(logit) 값이며, $$K$$는 클래스의 수를 의미합니다.

#### 수치적 불안정성 (Numerical Instability)
Softmax 함수는 지수화를 포함하기 때문에, 큰 값을 입력으로 받으면 수치적 불안정성이 발생할 수 있습니다. 수치적 불안정성은 계산 도중 오버플로우(overflow)나 언더플로우(underflow)가 발생하여 계산 결과가 무한대(또는 무한대에 가까운 값)로 변하거나 0이 되어버리는 현상을 말합니다.

예를 들어, 만약 입력 값이 매우 큰 경우, 지수화한 값이 너무 커져서 컴퓨터의 표현 범위를 초과할 수 있습니다. 반대로, 매우 작은 값이 들어오는 경우, 지수화 후 너무 작은 값이 되어 0에 가까운 값으로 계산될 수 있습니다. 이러한 현상은 계산의 정확도를 크게 떨어뜨립니다.

#### 수치적 불안정성 해결 방법
수치적 불안정성을 해결하기 위해 가장 일반적으로 사용되는 방법은 **입력 값의 최대값을 계산하여 모든 입력 값에서 이를 빼는 방법**입니다. 이 방법을 적용하면 다음과 같은 이점이 있습니다:

1. **오버플로우 방지**: 입력 값 중 가장 큰 값을 0으로 만들고 나머지 값들은 음수로 변환되므로, 지수화된 값이 너무 크게 되는 것을 방지할 수 있습니다.
   
2. **확률 분포 유지**: 모든 입력 값에 같은 상수를 빼도 Softmax 함수의 출력인 확률 분포에는 영향을 미치지 않습니다. 따라서 모델의 예측 성능에는 변화가 없습니다.

#### Softmax 수치적 안정화 과정
이를 수식으로 설명하면 다음과 같습니다:

$${softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

이제, 각 입력 값 $$z_i$$에서 최대값 $$z_{max}$$을 빼준 후 Softmax를 적용합니다:

$${softmax}(z_i) = \frac{e^{z_i - z_{\text{max}}}}{\sum_{j=1}^{K} e^{z_j - z_{\text{max}}}}$$

여기서 $$z_{\text{max}} = \max(z_1, z_2, \dots, z_K)$$입니다.

이 과정에서 $$z_i - z_{\text{max}}$$의 값은 0보다 작거나 같아지므로, 지수화된 값들이 매우 커지는 것을 방지하고, 더 안정적인 계산을 가능하게 합니다.
