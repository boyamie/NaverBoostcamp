---
title: "Boostcamp AI Tech (Day 006)"
date: 2024-08-12
layout: post
tags: [Naver Boostcamp, daily report]
---

[![Peer Session Badge](https://img.shields.io/badge/Peer%20Session-CC527A?style=flat)](../peer_session/day006.html)

# 1.강의듣기
## 1.Linear Classifier와 Loss Function
>리니어 분류기
- 리니어 분류기는 입력 데이터를 레이블 점수에 매핑하는 함수 `f`를 사용합니다.
- 마진 기반 손실과 교차 엔트로피 같은 손실 함수는 학습 과정에서 모델의 성능을 정량화합니다.
- 빠른 테스트와 적은 메모리 사용은 리니어 분류기의 주요 이점입니다.


- 리니어 분류기는 입력 데이터를 레이블 점수에 매핑하는 함수 `f`를 활용합니다.
- 매개변수적 접근을 통해 각 픽셀의 가중치 합계를 사용하여 클래스별 점수를 결정하며, 적절한 가중치 `W` 값을 설정해야 합니다.
- 손실 함수는 학습 과정에서 사용되며, 마진 기반 손실 및 교차 엔트로피 등의 방법이 있습니다.
- 리니어 분류는 빠른 테스트와 적은 메모리 사용이 장점입니다.

## 2.Softmax Classifier 
>소프트맥스 분류기
- 소프트맥스 분류기는 점수를 확률로 변환하여 클래스 간의 구분을 명확히 합니다.
- 점수는 0과 1 사이의 경계로 해석되어 특정 클래스에 속할 확률을 결정합니다.
- 점수 차이가 클수록 특정 클래스에 속할 확률이 높아지는 특징이 있습니다.
- Sigmoid 함수를 통해 클래스의 경계 점수도 정의할 수 있습니다.

- 소프트맥스 분류기는 점수의 유용성과 사용 방법을 설명합니다.
- 점수는 해석이 어려워서 0과 1 사이의 경계 점수를 얻어 확률로 해석할 수 있어야 합니다.
- Sigmoid 함수를 사용하여 클래스의 경계 점수를 정의하고, 소프트맥스 분류기가 확률을 올바르게 산출하는지 확인해야 합니다.

## 3.Loss Function의 정의와 종류 소개
>손실 함수
- 손실 함수는 모델의 성능을 정량화하고 패널티를 부여합니다.
- 예측이 정확할수록 손실이 감소하며, 크로스 엔트로피 손실은 예측 확률에 비례합니다.
- Kullback-Leibler 발산은 두 확률 분포의 차이를 측정하는 데 사용됩니다.

- 손실 함수는 머신러닝 모델의 성능을 정량화하며, 추정값과 기준값의 차이에 따라 패널티를 부여합니다.
- 마진 기반 손실에서는 예측이 정확할수록 손실이 작아지며, 잘못된 분류일 경우 패널티가 커집니다.
- 크로스 엔트로피 손실 함수는 예측 확률이 1에 가까워질수록 손실이 감소합니다.
- Kullback-Leibler 발산은 두 확률 분포의 차이를 측정합니다.

## 4.최적화의 필요성과 방법에 대한 설명
>최적화
- 최적화는 매개변수 `W`의 값을 결정하는 과정으로, 반복적으로 업데이트됩니다.
- 그레이디언트 강하법을 통해 비용 함수의 기울기를 계산하고 이를 바탕으로 매개변수를 조정합니다.
- 최적화 과정은 예측값이 기준값에 근접할 때까지 반복됩니다.
- 기계 학습에서는 스케일 문제를 다루기 위한 다양한 최적화 방법이 필요합니다.

- 최적화는 매개변수 `W`의 값을 결정하는 중요한 문제이며, 머신러닝은 데이터 기반 접근 방식입니다.
- 그레이디언트 강하법을 사용하면 비용 함수 `J(Θ)`의 기울기를 계산하여 최적화할 수 있습니다.

## 5.ML과 최적화 과정
- Gradient Descent 최적화에서는 로컬 최적점이나 새들 포인트가 존재할 수 있으며, 대규모 데이터셋에서는 시간이 오래 걸릴 수 있습니다.
- Stochastic Gradient Descent 최적화는 훈련 예제의 하위집합에 대해 무작위로 샘플링하여 효율적으로 기울기를 계산합니다.

# 2.퀴즈 오답노트

### Softmax 함수와 수치적 불안정성

#### Softmax 함수 개요
Softmax 함수는 머신러닝, 특히 다중 클래스 분류 문제에서 널리 사용되는 활성화 함수입니다. 주어진 입력 값들의 집합에서 각 값의 지수(exp)를 계산하고, 이 지수들의 합으로 나눈 값을 출력으로 반환합니다. 이 과정은 입력 값을 확률 분포로 변환하는 역할을 합니다. 예를 들어, 여러 클래스 중 특정 클래스에 속할 확률을 계산할 때 사용됩니다.

Softmax 함수의 정의는 다음과 같습니다:

$${softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

여기서 $$z_i$$는 클래스 $$i$$에 대한 로짓(logit) 값이며, $$K$$는 클래스의 수를 의미합니다.

#### 수치적 불안정성 (Numerical Instability)
Softmax 함수는 지수화를 포함하기 때문에, 큰 값을 입력으로 받으면 수치적 불안정성이 발생할 수 있습니다. 수치적 불안정성은 계산 도중 오버플로우(overflow)나 언더플로우(underflow)가 발생하여 계산 결과가 무한대(또는 무한대에 가까운 값)로 변하거나 0이 되어버리는 현상을 말합니다.

예를 들어, 만약 입력 값이 매우 큰 경우, 지수화한 값이 너무 커져서 컴퓨터의 표현 범위를 초과할 수 있습니다. 반대로, 매우 작은 값이 들어오는 경우, 지수화 후 너무 작은 값이 되어 0에 가까운 값으로 계산될 수 있습니다. 이러한 현상은 계산의 정확도를 크게 떨어뜨립니다.

#### 수치적 불안정성 해결 방법
수치적 불안정성을 해결하기 위해 가장 일반적으로 사용되는 방법은 **입력 값의 최대값을 계산하여 모든 입력 값에서 이를 빼는 방법**입니다. 이 방법을 적용하면 다음과 같은 이점이 있습니다:

1. **오버플로우 방지**: 입력 값 중 가장 큰 값을 0으로 만들고 나머지 값들은 음수로 변환되므로, 지수화된 값이 너무 크게 되는 것을 방지할 수 있습니다.
   
2. **확률 분포 유지**: 모든 입력 값에 같은 상수를 빼도 Softmax 함수의 출력인 확률 분포에는 영향을 미치지 않습니다. 따라서 모델의 예측 성능에는 변화가 없습니다.

#### Softmax 수치적 안정화 과정
이를 수식으로 설명하면 다음과 같습니다:

$${softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

이제, 각 입력 값 $$z_i$$에서 최대값 $$z_{max}$$을 빼준 후 Softmax를 적용합니다:

$${softmax}(z_i) = \frac{e^{z_i - z_{\text{max}}}}{\sum_{j=1}^{K} e^{z_j - z_{\text{max}}}}$$

여기서 $$z_{\text{max}} = \max(z_1, z_2, \dots, z_K)$$입니다.

이 과정에서 $$z_i - z_{\text{max}}$$의 값은 0보다 작거나 같아지므로, 지수화된 값들이 매우 커지는 것을 방지하고, 더 안정적인 계산을 가능하게 합니다.

# 3.assignment#1
업로드하신 두 번째 문서에서는 선형 회귀(Linear Regression)와 최근접 이웃 분류기(Nearest Neighbor Classifier)에 대한 이론적 개념과 예제 코드를 설명하고 있습니다. 각각의 주요 내용과 예제 코드를 간단히 설명드리겠습니다.

### 1. 선형 회귀 (Linear Regression)

#### 정의 및 개념:
- **선형 회귀**는 종속 변수와 하나 이상의 독립 변수 간의 관계를 모델링하는 통계적 방법입니다. 이 방법을 통해 독립 변수의 값에 따라 종속 변수의 값을 예측할 수 있습니다.
- 선형 회귀 방정식: \( y = mx + b \), 여기서 \( y \)는 종속 변수, \( x \)는 독립 변수, \( m \)은 직선의 기울기(회귀 계수), \( b \)는 y 절편(상수항)입니다.

#### 가정:
- **선형성**: 종속 변수와 독립 변수 간의 관계가 선형적이어야 합니다.
- **독립성**: 관측값들이 서로 독립적이어야 합니다.
- **등분산성**: 오류의 분산이 일정해야 합니다.
- **정규성**: 오류가 정규 분포를 따라야 합니다.

#### 모델 평가 지표:
- **평균 절대 오차(MAE)**: 예측값과 실제값 간의 절대 차이를 평균화한 지표.
- **평균 제곱 오차(MSE)**: 예측값과 실제값 간의 차이의 제곱을 평균낸 값으로, 큰 오차에 민감합니다.
- **제곱근 평균 제곱 오차(RMSE)**: MSE에 제곱근을 취하여 실제값과 같은 단위를 유지하는 지표.
- **결정계수(R²)**: 모델이 종속 변수의 변동성을 얼마나 설명하는지를 나타내는 지표입니다.

### 2. 최근접 이웃 분류기 (Nearest Neighbor Classifier)

#### 개념
- **Nearest Neighbor Classifier**는 주어진 쿼리 데이터 포인트에 대해 가장 가까운 \( k \)개의 학습 데이터 포인트를 찾아서 그들의 레이블을 사용하여 예측하는 방법입니다.

#### 예제 코드:
```python
import numpy as np

class NearestNeighbor:
    def __init__(self):
        pass

    def train(self, images, labels):
        # 모든 학습 데이터를 기억
        self.images = images
        self.labels = labels

    def predict(self, test_image):
        # 테스트 이미지에 대해 가장 유사한 학습 이미지를 찾음
        min_dist = sys.maxsize  # Python에서 사용할 수 있는 최대 정수
        for i in range(self.images.shape[0]):
            dist = np.sum(np.abs(self.images[i, :] - test_image))
            if dist < min_dist:
                min_dist = dist
                min_index = i

        return self.labels[min_index]
```

#### 코드 설명:
- **train 메서드**: 학습 데이터와 해당 레이블을 기억합니다.
- **predict 메서드**: 테스트 이미지와 학습 데이터 사이의 거리를 계산하여 가장 유사한 학습 이미지를 찾고, 그 이미지의 레이블을 반환합니다.

이 코드는 기본적인 Nearest Neighbor Classifier를 구현한 것으로, 간단한 알고리즘이지만, 고차원 데이터나 대규모 데이터셋에 적용하기에는 비효율적일 수 있습니다.

이 문서에서는 예제 코드를 통해 머신 러닝의 기본 개념과 간단한 알고리즘을 실습해볼 수 있도록 구성되어 있습니다. 더 복잡한 알고리즘이나 개선된 방법은 후속 강의에서 다루어질 수 있습니다.

업로드하신 세 번째 문서에서는 선형 분류기(Linear Classifier)와 소프트맥스 분류기(Softmax Classifier)에 대한 이론과 예제 코드가 설명되어 있습니다. 아래는 주요 내용을 요약하고 예제 코드를 설명드리겠습니다.

### 1. 선형 분류기 (Linear Classifier)

#### 개념:
- **선형 분류기**는 입력 데이터를 레이블 점수(클래스)에 매핑하는 함수 \( f(x) \)를 사용하는 모델입니다. 이 모델에서는 입력 \( x \)와 가중치 \( W \)의 선형 결합으로 결과를 계산합니다.
- 선형 결합은 각 픽셀에 가중치 \( W \)를 곱한 후 모두 더해지는 방식으로 이루어집니다. 이는 수학적으로 \( f(x) = Wx + b \)로 표현됩니다.

#### 예제:
- 각 클래스마다 독립적인 가중치 벡터 \( W \)를 학습하며, 이 벡터는 해당 클래스에 대한 점수를 계산하는 데 사용됩니다.
- 예를 들어, 32x32 크기의 RGB 이미지(즉, 3072차원 벡터)를 입력으로 받는 경우, 각 클래스에 대해 3072개의 가중치가 존재합니다.

### 2. 소프트맥스 분류기 (Softmax Classifier)

#### 개념:
- 선형 분류기의 출력 점수는 확률로 해석하기 어려우므로, 소프트맥스 함수를 적용해 점수를 확률로 변환합니다.
- **소프트맥스 함수**는 모든 클래스의 출력 점수에 대해 확률을 계산하며, 이 확률들의 합은 항상 1이 됩니다. 이는 모델이 특정 클래스에 속할 확률을 예측하는 데 사용됩니다.

### 3. 손실 함수 (Loss Function)

#### 개념:
- 손실 함수는 모델의 예측이 실제 레이블과 얼마나 다른지를 정량화합니다.
- **크로스 엔트로피 손실(Cross Entropy Loss)**는 예측 확률이 실제 레이블과 얼마나 가까운지를 계산하며, 모델이 잘못된 예측을 할수록 손실이 커지도록 합니다.

### 4. 최적화 (Optimization)

#### 개념:
- **최적화**는 손실 함수를 최소화하는 가중치 \( W \)를 찾는 과정입니다. 일반적으로 **경사 하강법(Gradient Descent)**이 사용되며, 손실 함수의 기울기를 따라 가중치를 업데이트합니다.
- **확률적 경사 하강법(Stochastic Gradient Descent)**은 전체 데이터셋이 아닌 랜덤하게 선택된 데이터의 하위 집합을 사용해 가중치를 업데이트하는 방식입니다.

### 예제 코드 설명

문서에서 제공된 예제 코드 부분은 명시적으로 포함되어 있지는 않지만, 전체적인 과정은 다음과 같은 코드 구조를 가질 수 있습니다:

```python
import numpy as np

# 선형 분류기 클래스 정의
class LinearClassifier:
    def __init__(self):
        pass

    # 모델 학습 함수
    def train(self, X, y):
        # X: 학습 데이터 (N x D 행렬)
        # y: 레이블 (N차원 벡터)
        num_classes = np.max(y) + 1  # 클래스의 수
        num_features = X.shape[1]
        
        # 가중치 초기화
        self.W = np.random.randn(num_classes, num_features) * 0.001

    # 예측 함수
    def predict(self, X):
        # X: 테스트 데이터 (M x D 행렬)
        scores = np.dot(X, self.W.T)  # 점수 계산
        predictions = np.argmax(scores, axis=1)  # 최대 점수를 가지는 클래스 예측
        return predictions
```

### 코드 설명:
- `train` 함수는 입력 데이터 \( X \)와 레이블 \( y \)를 받아 가중치 \( W \)를 학습합니다.
- `predict` 함수는 테스트 데이터 \( X \)에 대해 각 클래스에 대한 점수를 계산한 후, 가장 높은 점수를 가진 클래스를 예측합니다.
- 이 코드 구조는 선형 분류기와 소프트맥스 분류기를 구현하는 데 기초가 되며, 손실 함수와 최적화 과정을 추가하여 모델의 성능을 개선할 수 있습니다.

이러한 코드와 개념들은 기초적인 머신 러닝 모델 구현에 중요한 역할을 하며, 실제 프로젝트에 적용할 때는 다양한 최적화 기법과 손실 함수를 사용해 모델의 성능을 더욱 향상시킬 수 있습니다.
