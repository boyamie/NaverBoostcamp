---
title: "Boostcamp AI Tech (Day 007)"
date: 2024-08-13
layout: post
tags: [Naver Boostcamp, daily report]
---

[![Peer Session Badge](https://img.shields.io/badge/Peer%20Session-CC527A?style=flat)](../peer_session/day007.html)  

### 선형 분류와 Bias Vector

**1. 선형 분류(Linear Classification)**
- **정의:** linear classifier에서 bias vector(b) 값은 데이터 셋이  편향되었을 때 사용되는 상수 항을 의미합니다. 선형 분류는 데이터를 특정 기준으로 나누기 위해 선형 결정 경계(직선 또는 초평면)를 사용하는 분류 방법입니다. 주로 Logistic Regression, Support Vector Machine(SVM) 등이 대표적입니다.
- **모델 수식:** 선형 분류 모델
  
  $$y = \mathbf{W} \cdot \mathbf{X} + \mathbf{b}$$

  여기서 $$\mathbf{W}$$는 가중치 벡터, $$\mathbf{X}$$는 입력 데이터 벡터,$$\mathbf{b}$$는 bias 벡터입니다.

**2. Bias Vector**
- **역할:** Bias는 모델의 출력에 일정한 오프셋을 추가하는 역할을 합니다. 이는 데이터를 특정 방향으로 이동시키며, 모델이 데이터를 더 잘 맞출 수 있도록 돕습니다.
- **오답노트:** Bias vector가 데이터와 라벨 사이의 숨겨진 관계를 나타낸다는 설명은 잘못된 것입니다. 실제로, bias는 단순히 결정 경계가 지나가는 위치를 조정하는 역할을 합니다. 숨겨진 관계는 가중치 $$\mathbf{W}$$에 의해 주로 표현됩니다.

### Logistic Regression과 Neural Network

**1. Logistic Regression**
- **정의:** Logistic Regression은 이진 분류 문제를 해결하기 위한 통계적 모델입니다. 선형 모델을 사용하여 데이터를 분류하고, sigmoid 함수를 통해 이진 클래스에 대한 확률을 출력합니다.
  
  $$p(y=1|\mathbf{X}) = \sigma(\mathbf{W} \cdot \mathbf{X} + \mathbf{b})$$

  여기서 $$\sigma(z) = \frac{1}{1 + e^{-z}}$$는 sigmoid 함수입니다.

- **특징:** Logistic Regression은 선형 분류기이며, 출력이 0과 1 사이의 확률로 해석됩니다.

**2. Logistic Regression과 Neural Network**
- **특별한 경우:** Logistic Regression은 신경망의 매우 단순한 형태로 볼 수 있습니다. 신경망은 일반적으로 여러 개의 뉴런(가중치와 bias를 포함)과 비선형 활성화 함수를 가진 레이어들로 구성됩니다. Logistic Regression은 단일 뉴런과 sigmoid 활성화 함수를 사용하는 단일 레이어 신경망입니다.
- **구조:** Logistic Regression의 구조는 다음과 같습니다:
  
  - 입력층(Input Layer): 특징 벡터 $$\mathbf{X}$$
  - 출력층(Output Layer): sigmoid 함수가 적용된 단일 출력 뉴런
 
  
RNN 모델에 대한 장단점, 기울기 소실 및 기울기 폭발 문제를 해결하는 방법, 그리고 LSTM과 Seq2Seq 모델
### RNN 모델의 장단점
**장점:**
- 시퀀스 데이터(예: 시계열, 텍스트) 처리에 적합합니다.
- 이전 입력에 대한 문맥을 유지할 수 있습니다.
- 입력과 출력의 길이가 가변적입니다.

**단점:**
- 긴 시퀀스에 대한 학습이 어려움.
- 기울기 소실(Vanishing Gradient) 및 기울기 폭발(Exploding Gradient) 문제에 취약합니다.
- 순차적으로 처리하므로 학습 속도가 느립니다.

### 기울기 소실 및 기울기 폭발 문제
- **기울기 소실:** 역전파 과정에서 기울기가 매우 작아져 가중치 업데이트가 거의 이루어지지 않는 문제입니다.
- **기울기 폭발:** 기울기가 너무 커져 가중치가 매우 큰 값으로 변하면서 모델이 불안정해지는 문제입니다.

**해결 방법:**
- **LSTM:** 게이트 구조를 통해 정보를 효과적으로 관리하여 긴 시퀀스의 의존성을 학습할 수 있습니다.
- **기울기 클리핑:** 기울기의 크기를 제한하여 기울기 폭발을 방지합니다.
- **배치 정규화:** 입력을 정규화하여 기울기 폭발 가능성을 줄입니다.

### LSTM과 Seq2Seq 모델
- **LSTM:** 긴 시퀀스의 장기 의존성을 학습할 수 있는 RNN의 변형입니다. 입력, 출력, 망각 게이트를 통해 정보를 효율적으로 조절합니다.
- **Seq2Seq:** 주로 기계 번역과 같은 작업에서 사용되는 모델 구조로, 인코더와 디코더로 구성되어 있습니다. 인코더는 입력 시퀀스를 컨텍스트 벡터로 변환하고, 디코더는 이를 바탕으로 출력 시퀀스를 생성합니다.

### PyTorch 기반 Seq2Seq 모델 코드
인코더는 입력 시퀀스를 받아 컨텍스트 벡터를 생성하고, 디코더는 이 벡터를 바탕으로 출력 시퀀스를 생성합니다. 학습 과정에서 "Teacher Forcing" 기법을 사용하여 학습 속도를 향상시킬 수 있습니다.
```python
import torch
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, src):
        embedded = self.dropout(self.embedding(src))
        outputs, (hidden, cell) = self.rnn(embedded)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input, hidden, cell):
        input = input.unsqueeze(0)
        embedded = self.dropout(self.embedding(input))
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(0))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        hidden, cell = self.encoder(src)
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        outputs = torch.zeros(trg_len, trg_vocab_size).to(self.device)
        input = trg[0, :]
        
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[t] = output
            top1 = output.argmax(1)
            input = trg[t] if torch.rand(1).item() < teacher_forcing_ratio else top1
        
        return outputs
```
